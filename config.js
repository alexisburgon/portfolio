module.exports = {
    templateDir: "./templates",
    baseTemplate: 'base.html',
    buildDir: './build',
    overallTitle: 'Alexis Burgon',
    aboutMe: "Machine Learning & Quality Assurance Engineer \n Building testing frameworks and validation tools for regulatory compliance \n Python, javascript, SQL",
    pages: [
        // Templated files
        {pageTitle: 'Alexis Burgon', file: 'index.html'},
        {pageTitle: 'Alexis Burgon - Projects', file: 'projects.html', displayName:'Projects'},
        {pageTitle: 'Alexis Burgon - Publications', file:'publications.html', displayName:'Publications'},
        // Static files
        {file: 'style.css', noTemplate:true}
    ],
    projects: [
        {name: 'DRAGen', link:'https://github.com/DIDSR/DRAGen', image:'DRAGen-flowchart.png', description:"AI/ML generalizability assessment through decision region analysis."},
        {name: 'bias.myti.report', link:'https://github.com/DIDSR/bias.myti.report', image:'bias-myti-visual-abstract.png', description:"A tool that facilitates the evaluation and comparison of bias mitigation methods thorough two novel bias amplification approaches."},
    ],
    contactInformation: [
        {icon: 'linkedin', link:'', text:'LinkedIn'},
        {icon: 'google-scholar', link:'https://scholar.google.com/citations?user=sfrd0esAAAAJ&hl=en', text:'Google Scholar'},
    ],
    publicationFile: './publications.bib',
    additionalPublicationInfo: {
        'kanakaraj2025behype': {
            url: 'https://doi.org/10.1117/12.3047359',
            abstract: 'Artificial intelligence (AI) models need to be carefully evaluated for performance on underrepresented subgroups to avoid exacerbating health disparities, but test data for such subgroups are often limited. Traditional evaluation methods often misinterpret performance differences across such limited subgroups data. We present an novel approach for meaningful subgroup analysis, based on hyperdimensional computing to encode model features during the AI model evaluation phase. The hyperdimensional representation retains the subtle subgroup characteristics and enables identification of diverging characteristics (DCs) responsible for performance differences across subgroups. Thus, we develop a technique to identify and detect these DCs and show that they reflect performance bias.'
        },
        'akhonda2025label': {
            url: 'https://doi.org/10.1117/12.3047437',
            abstract: 'Traditional bias correction methods in medical imaging, whether through post-training adjustments, data pre-processing, or in-training techniques, typically rely on labeled information such as age, gender, or ethnicity. This reliance may lead to overlooking subtle biases that arise from less obvious data variations or demographic factors and fails to utilize clinically relevant information that may not be labeled. Our proposed approach addresses these gaps by integrating optimal transport (OT)-based bias mitigation directly into the AI model training process without the requirement of labeled information. This method measures and aligns feature distributions within a class by leveraging their inherent similarities, capturing nuanced discrepancies that conventional approaches might miss.'
        },
        'burgon2024bias': {
            url: 'https://doi.org/10.1109/JBHI.2024.3491946',
            abstract: 'The future of artificial intelligence (AI) safety is expected to include bias mitigation methods from development to application. The complexity and integration of these methods could grow in conjunction with advances in AI and human-AI interactions. Numerous methods are being proposed to mitigate bias, but without a structured way to compare their strengths and weaknesses. In this work, we present two approaches to systematically amplify subgroup performance bias. These approaches allow for the evaluation and comparison of the effectiveness of bias mitigation methods on AI models by varying the degrees of bias, and can be applied to any classification model. We used these approaches to compare four off-the-shelf bias mitigation methods. Both amplification approaches promote the development of learning shortcuts in which the model forms associations between patient attributes and AI output. We demonstrate these approaches in a case study, evaluating bias in the determination of COVID status from chest x-rays. The maximum achieved increase in performance bias, measured as a difference in predicted prevalence, was 72% and 32% for bias between subgroups related to patient sex and race, respectively. These changes in predicted prevalence were not accompanied by substantial changes in the differences in subgroup area under the receiver operating characteristic curves, indicating that the increased bias is due to the formation of learning shortcuts, not a difference in ability to distinguish positive and negative patients between subgroups.'
        },
        'burgon2023predicting': {
            url: 'https://www.fda.gov/science-research/fda-science-forum/predicting-ai-model-behavior-unrepresented-subgroups-test-time-approach-increase-variability-finite',
        },
        'najarian2023semantic': {
            // No easy link   
        },
        'zhang2023evaluation': {
            // No easy link
        },
        'burgon2023methods': {
            // No easy link
        },
        'burgon2024manipulation': {
            url: 'https://doi.org/10.1117/12.3008267',
            abstract: 'The increased use of artificial intelligence (AI)-enabled medical devices in clinical practice has driven the need for better understanding of bias in AI applications. While there are many bias mitigation methods, determining which method is appropriate in each situation is not trivial due to a lack of objective bias mitigation assessment approaches. This study presents an approach to manipulate sources of bias in order to facilitate the evaluation of bias mitigation methods during AI development. This approach amplifies sources of bias in the data by varying disease prevalence between subgroups to promote associations between patient subgroups and specific disease classes. The approach can be adjusted to affect the degree to which bias is amplified. In this study, bias amplification was used to vary the bias of COVID-19 classification models using chest X-ray data between patient subgroups defined by sex (female or male) or race (black or white). Analysis of subgroup sensitivity shows that the proposed bias amplification approach results in the sensitivity of one subgroup increasing while the sensitivity of the other subgroup is decreasing, despite a consistent difference in subgroup area under the receiver operating characteristic curve. For example, the amplification of sex-related bias increased the sensitivity for COVID-19 classification of the female subgroup from 0.65±0.04 to 0.90±0.02 while decreasing the sensitivity of the male subgroup from 0.66±0.03 to 0.18±0.02. This shows that the bias amplification approaches promote bias by causing the model to enhance existing correlations between a patient’s subgroup and their COVID-19 status, which results in biased performance in a clinical setting. The approach presented in this study was found to systematically amplify sources of bias for our dataset and AI model, and can thus be used to evaluate the effectiveness of potential bias mitigation methods.'
        },
        'burgon2024tool': {
            url: 'https://doi.org/10.1117/12.3008580',
            abstract: 'Proper assessment of the ability of an artificial intelligence (AI)-enabled medical device to generalize to new patient populations is necessary to determine the safety and effectiveness of the device. Assessing AI generalizability relies on performance assessment on a data set which represents the device’s intended population, which can be challenging to obtain. An understanding of the AI model’s decision space can indicate how the device is likely to perform on patients not represented in the available data. Our tool for decision region analysis for generalizability (DRAGen) assessment estimates the composition of the region of the decision space surrounding the available data. This provides an indication of how the model is likely to perform on samples which are similar to, but not represented by, the available finite data set. DRAGen can be applied to any binary classification model and requires no knowledge of the model’s training process. In a case study, we demonstrated DRAGen on a COVID classification model and showed that the decision region composition can identify differences in correct classification rates between the positive and negative classes, even with comparable performance on the original test set. Performance evaluation using a data set which was not represented during model development nor within the original test set shows a disparity in the performance between COVID-positive and COVID-negative patients, as indicated by DRAGen. By releasing this tool, we encourage future AI developers to use our tool to improve understanding of generalizability. '
        },
        'burgon2024decision': {
            url: 'https://doi.org/10.1117/1.JMI.11.1.014501',
            abstract: 'Understanding an artificial intelligence (AI) model’s ability to generalize to its target population is critical to ensuring the safe and effective usage of AI in medical devices. A traditional generalizability assessment relies on the availability of large, diverse datasets, which are difficult to obtain in many medical imaging applications. We present an approach for enhanced generalizability assessment by examining the decision space beyond the available testing data distribution. Vicinal distributions of virtual samples are generated by interpolating between triplets of test images. The generated virtual samples leverage the characteristics already in the test set, increasing the sample diversity while remaining close to the AI model’s data manifold. We demonstrate the generalizability assessment approach on the non-clinical tasks of classifying patient sex, race, COVID status, and age group from chest x-rays. Decision region composition analysis for generalizability indicated that a disproportionately large portion of the decision space belonged to a single “preferred” class for each task, despite comparable performance on the evaluation dataset. Evaluation using cross-reactivity and population shift strategies indicated a tendency to overpredict samples as belonging to the preferred class (e.g., COVID negative) for patients whose subgroup was not represented in the model development data. An analysis of an AI model’s decision space has the potential to provide insight into model generalizability. Our approach uses the analysis of composition of the decision space to obtain an improved assessment of model generalizability in the case of limited test data.'
        },
        'najarian2024effect': {
            url: 'https://doi.org/10.1117/12.3008265',
            abstract: "Many artificial intelligence algorithms are currently deployed in medicine to help decision making and treatment planning. As new data become available, these algorithms can be updated with the goal to increase performance and improve patient outcomes. One method for updating algorithms is incrementally training on newly available data in multiple steps following sequential training protocols. For segmentation algorithms, changing the class labels of background pixels is known as semantic distribution shift, which can decrease algorithm knowledge retention. In this work, we explore the effects of semantic distribution shift on the knowledge retention of sequentially trained tumor segmentation algorithms by systematically altering the reference standards to simulate annotations by different annotators. Algorithms were trained in two sequential steps. The first step used an unmodified reference standard and the second step training data had a modified reference standard. The modified reference standards, which simulate annotator over- and under-segmentation, were created by systematically dilating or eroding the reference standard with different kernel sizes. A baseline algorithm is trained with unmodified data in both steps. Two types of variability are explored. The first is homogeneous variability, where the modified reference standards are changed in a consistent manner; for example, all reference standard segmentations were dilated or eroded with the same dilation or erosion structural element. The second is heterogeneous variability, in which the modified training data contains a mixture of dilated and eroded reference standards. Algorithm performance is evaluated using the Dice Coefficient, 95% Hausdorff Distance, and Volume Distance Coefficient, an adaptation of the Volume Similarity Coefficient. For the gadolinium-enhancing tumor subregion, algorithms trained with homogeneous variability had Dice values ranging from 0.38±0.01 to 0.76±0.02, while algorithms trained with heterogeneous variability had Dice values ranging from 0.76±0.02 to 0.79±0.01, much closer to the baseline algorithm’s value of 0.81±0.03. The results show that semantic distribution shift due to homogeneous variability decreases algorithm knowledge retention more than the shift due to heterogeneous variability. In practice, this suggests that the potential decrease in knowledge retention resulting from the addition of new data can be minimized by ensuring that the new data comes from a diverse variety of sources. "
        },
        'burgon2023decision': {
            url: 'https://doi.org/10.1117/12.2653963',
            abstract: 'Assessing the generalizability of deep learning algorithms based on the size and diversity of the training data is not trivial. This study uses the mapping of samples in the image data space to the decision regions in the prediction space to understand how different subgroups in the data impact the neural network learning process and affect model generalizability. Using vicinal distribution-based linear interpolation, a plane of the decision region space spanned by the random ‘triplet’ of three images can be constructed. Analyzing these decision regions for many random triplets can provide insight into the relationships between distinct subgroups. In this study, a contrastive self-supervised approach is used to develop a ‘base’ classification model trained on a large chest x-ray (CXR) dataset. The base model is fine-tuned on COVID-19 CXR data to predict image acquisition technology (computed radiography (CR) or digital radiography (DX) and patient sex (male (M) or female (F)). Decision region analysis shows that the model’s image acquisition technology decision space is dominated by CR, regardless of the acquisition technology for the base images. Similarly, the Female class dominates the decision space. This study shows that decision region analysis has the potential to provide insights into subgroup diversity, sources of imbalances in the data, and model generalizability. '
        },
    }
}